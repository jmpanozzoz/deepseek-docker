#!/bin/bash

export HF_TOKEN=$HUGGING_FACE_TOKEN
QUANTIZATION=${QUANTIZATION:-None}  # Usar None si no est치 definido

echo "游댳 Usando QUANTIZATION: $QUANTIZATION"
echo "游댳 Descargando modelo: $MODEL_NAME"

# Verificar si `QUANTIZATION` es v치lido
VALID_QUANTIZATIONS=("aqlm" "awq" "fp8" "gptq" "squeezellm" "marlin" "None")
if [[ ! " ${VALID_QUANTIZATIONS[@]} " =~ " ${QUANTIZATION} " ]]; then
    echo "丘멆잺 Error: Valor inv치lido para QUANTIZATION (${QUANTIZATION})"
    echo "   Opciones v치lidas: ${VALID_QUANTIZATIONS[*]}"
    exit 1
fi

# Descargar modelo sin aplicar cuantizaci칩n si es "None"
if [[ "$QUANTIZATION" == "None" ]]; then
    QUANTIZATION_ARG=""
else
    QUANTIZATION_ARG="revision='${QUANTIZATION}',"
fi

python3 -c "
from huggingface_hub import snapshot_download
snapshot_download(
    '${MODEL_NAME}',
    ${QUANTIZATION_ARG}
    cache_dir='/root/.cache/huggingface/hub',
    ignore_patterns=['*.safetensors', '*.bin']
)
"

# Detectar Compute Capability (a칰n no se usa, pero puede ser 칰til)
COMPUTE_CAP=$(nvidia-smi --query-gpu=compute_cap --format=csv,noheader | head -1 | tr -d '.')

echo "游 Iniciando vLLM"

# Iniciar servidor
exec python3 -m vllm.entrypoints.api_server \
    --model "${MODEL_NAME}" \
    --quantization "${QUANTIZATION}" \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.97 \
    --max-model-len 8192 \
    --max-num-batched-tokens 7168 \
    --max-parallel-loading-workers 4
